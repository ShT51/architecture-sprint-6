## 1. Определите стратегию масштабирования и отказоустойчивости
Горизонтальное масштабирование будет предпочтительным, так как оно позволяет более гибко управлять нагрузкой и обеспечивает лучшую отказоустойчивость. 
Вертикальное масштабирование имеет ограничения по максимальной мощности одного сервера и не обеспечивает достаточной отказоустойчивости.

Использование нескольких зон доступности необходимо для обеспечения высокой доступности и отказоустойчивости. 
Это позволит распределить нагрузку и минимизировать риски простоя в случае сбоя в одной из зон.

## 2. Развёртывание приложения в Kubernetes
Независимые кластеры будут предпочтительнее, т.к. данный подход снижает зависимость от сетевой связанности между ЦОД и от внутренних недостатков фейловер-механизмов в Kubernetes

## 3. Балансировка нагрузки
- **Global Server Load Balancing** (GSLB) будет использоваться для распределения трафика между географически распределёнными серверами. 
Это обеспечит минимальное время отклика для пользователей из разных регионов.
- **Health checks** будут настроены на уровне Kubernetes и GSLB для мониторинга состояния сервисов и автоматического перенаправления трафика в случае сбоев.

## 4. Фейловер-стратегия
**Active-Active** конфигурация будет использоваться для обеспечения высокой доступности.  
В случае сбоя в одной зоне, трафик автоматически перенаправляется на другие зоны. 
Это обеспечит выполнение требований RTO и RPO.

## 5. Конфигурация базы данных
- Для репликации будет использоваться **Patroni** для управления кластером. 
Это обеспечит автоматическое переключение на резервную реплику в случае сбоя основной базы данных.
- Регулярные резервные копии будут выполняться с использованием инструментов, таких как **pgBackRest**, для обеспечения быстрого восстановления данных в случае сбоя.

## 6. Шардирование БД
На данном этапе шардирование не требуется, но с ростом кол-ва пользователей и объема данных (сейчас храниться только базовая информация о клиентах) шардирование будет необходимо.  

Предложил бы использовать расширение **Citus**, ключом шардирования выступал бы уникальный идентификатор пользователя.
Но т.к. шардирование БД накладывает определенные ограничения и требует определенных подходов как при построении инфраструктуры, как при разработке архитектуры таблиц, так и при написании кода приложений,
я бы внедрил его уже на этом этапе, но с минимальным кол-вом узлов кластера Citus а, по мере нагрузки, добавлял бы узлы в кластер.